{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotheken/Einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1 = pd.read_parquet(\"../Daten/energy_test1.parquet\")\n",
    "df_test2 = pd.read_parquet(\"../Daten/energy_test2.parquet\")\n",
    "df_train = pd.read_parquet(\"../Daten/energy_train.parquet\")\n",
    "df_forecasts = pd.read_parquet(\"../Daten/forecasts.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beobachtung der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Zeilen anpassen (durchschnitt, vorheriger Wert) oder löschen\n",
    "df_train[df_train.Solar_MWh.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train[df_train.Solar_MWh.isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ref_datetime', 'valid_time', 'SolarDownwardRadiation', 'CloudCover',\n",
       "       'Temperature', 'Weather Model', 'valid_datetime'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_forecasts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecasts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecasts[df_forecasts.SolarDownwardRadiation.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Umgang mit NaN-Werten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Umgang mit NaN-Werten in df_train__\n",
    "- Betroffene Zeilen 4 von 19968 (ca.0,02%)\n",
    "- Mein Ansatz: Zeilen, wo bei `Solar_MWh` NaN auftaucht, löschen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.dropna()\n",
    "df_test1 = df_test1.dropna()\n",
    "df_test2 = df_test2.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Umgang mit NaN-Werten in df_forecasts**\n",
    "- Betroffene Zeilen: max. 1226 von 606797 (ca. 0,2%)\n",
    "- Mein Ansatz: Daten behalten und Auffüllen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spalte SolarDownwardRadiation\n",
    "# df_forecasts[\"SolarDownwardRadiation\"] = df_forecasts[\"SolarDownwardRadiation\"].interpolate(method=\"linear\")\n",
    "\n",
    "# # Spalte CloudCover\n",
    "# df_forecasts[\"CloudCover\"] = df_forecasts[\"CloudCover\"].fillna(df_forecasts[\"CloudCover\"].median())\n",
    "\n",
    "# # Spalte Temperature\n",
    "# df_forecasts[\"Temperature\"] = df_forecasts[\"Temperature\"].interpolate(method=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecasts = df_forecasts.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validierung nach der Bereinigung**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NaN-Werte in df_train:\", df_train.isna().sum())\n",
    "print(\"NaN-Werte in df_forecasts:\", df_forecasts.isna().sum())\n",
    "print(\"NaN-Werte in df_train:\", df_test1.isna().sum())\n",
    "print(\"NaN-Werte in df_train:\", df_test2.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Umgang mit Negativen Werten `SolarDownwardRadiation` (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative SolarDownwardRadiation anpassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecasts[\"SolarDownwardRadiation\"] = df_forecasts[\"SolarDownwardRadiation\"].clip(lower=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 1 Untersuchung der Energiedaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wie viele Datenpunkte, die vorhergesagt werden sollen, gibt es in den Trainings- bzw. Testdaten?\n",
    "train_points = len(df_train)\n",
    "\n",
    "# Anzahl der Datenpunkte in den beiden Testsets\n",
    "test1_points = len(df_test1)\n",
    "test2_points = len(df_test2)\n",
    "\n",
    "print(f\"Trainingsdatenpunkte: {train_points}\")\n",
    "print(f\"Testdatenpunkte - Test1: {test1_points}, Test2: {test2_points}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Tagesverlauf visualisieren\n",
    "# (Zufälige)Geburtstage auswählen, bzw. Frühlings-, Sommer- und Winterzeit\n",
    "birthdates = [\"2022-05-21\", \"2022-07-28\", \"2022-12-16\"]\n",
    "birthdates = pd.to_datetime(birthdates) # Strings in datetime umwandeln\n",
    "\n",
    "df_train[\"date\"] = pd.to_datetime(df_train[\"dtm\"]).dt.date  # Extrahiere das Datum\n",
    "\n",
    "# Filter für die ausgewählten Tage\n",
    "filtered_data = df_train[df_train[\"date\"].isin(birthdates.date)]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "for date in birthdates:\n",
    "    daily_data = filtered_data[filtered_data[\"date\"] == date.date()]\n",
    "    plt.plot(\n",
    "        pd.to_datetime(daily_data[\"dtm\"]).dt.hour,\n",
    "        daily_data[\"Solar_MWh\"],\n",
    "        label=str(date.date())\n",
    "    )\n",
    "plt.xlabel(\"Stunde des Tages\")\n",
    "plt.ylabel(\"Stromerzeugung (Solar_MWh)\")\n",
    "plt.title(\"Tagesverlauf der Stromerzeugung\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022-05-21  Frühling <br>\n",
    "2022-07-28 -> Sommer <br>\n",
    "2022-12-16 -> Winter <br>\n",
    "Man erkennt deutlich, dass logischerweise Nachts kein Strom produziert wird und der Tag der wichtige Teil für die Stromerzeugung. Auch erkennbar ist, dass der Winter weniger Strom produziert, was sehr stark an der Wolkenbedeckung liegen kann und die Tageszeit kürzer anhält als Frühling und Sommer.\n",
    "Einen kleinen Ausreißer erkennt man beim Frühling, was bei ca. 10-12Uhr kurz wenig Strom produziert. Das könnte an einem Regenschauer liegen oder anderen technischen Fehlern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Gesamtverlauf visualisieren\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(pd.to_datetime(df_train[\"dtm\"]), df_train[\"Solar_MWh\"], color=\"blue\")\n",
    "plt.xlabel(\"Zeit\")\n",
    "plt.ylabel(\"Stromerzeugung (Solar_MWh)\")\n",
    "plt.title(\"Gesamtverlauf der Stromerzeugung (Trainingsdaten)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Sommer steigt die Stromerzeugung, da es länger hell bleibt und wenig Wolkenbedeckung hat. <br>\n",
    "Im Winter sinkt die Stromerzeugung, da es schneller dunkel wird und weniger Sonnenschein tagsüber hat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 2 Merge und Untersuchung von Zusammenhänge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge mit Trainingsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konvertiere `ref_datetime` und `dtm` zu datetime-Objekten\n",
    "df_train[\"ref_datetime\"] = pd.to_datetime(df_train[\"ref_datetime\"])\n",
    "df_test1[\"ref_datetime\"] = pd.to_datetime(df_test1[\"ref_datetime\"])\n",
    "df_test2[\"ref_datetime\"] = pd.to_datetime(df_test2[\"ref_datetime\"])\n",
    "df_train[\"dtm\"] = pd.to_datetime(df_train[\"dtm\"])\n",
    "df_test1[\"dtm\"] = pd.to_datetime(df_test1[\"dtm\"])\n",
    "df_test2[\"dtm\"] = pd.to_datetime(df_test2[\"dtm\"])\n",
    "\n",
    "# Gültigen Zeitpunkt für Wettervorhersagen berechnen\n",
    "df_forecasts[\"valid_datetime\"] = df_forecasts[\"ref_datetime\"] + pd.to_timedelta(df_forecasts[\"valid_time\"], unit=\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(\n",
    "    df_train,\n",
    "    df_forecasts,\n",
    "    left_on=[\"dtm\", \"ref_datetime\"],\n",
    "    right_on=[\"valid_datetime\", \"ref_datetime\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "df_merged_test1 = pd.merge(\n",
    "    df_test1,\n",
    "    df_forecasts,\n",
    "    left_on=[\"dtm\", \"ref_datetime\"],\n",
    "    right_on=[\"valid_datetime\", \"ref_datetime\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "df_merged_test2 = pd.merge(\n",
    "    df_test2,\n",
    "    df_forecasts,\n",
    "    left_on=[\"dtm\", \"ref_datetime\"],\n",
    "    right_on=[\"valid_datetime\", \"ref_datetime\"],\n",
    "    how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test1 Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_test1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test2 Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineered Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot erstellung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Plot erstellen\n",
    "# Scatterplots für jede Wettervariable vs. Solar_MWh\n",
    "weather_attributes = [\"SolarDownwardRadiation\", \"CloudCover\", \"Temperature\"]\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "for i, attr in enumerate(weather_attributes):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    sns.scatterplot(data=df_merged, x=attr, y=\"Solar_MWh\", alpha=0.5)\n",
    "    plt.title(f\"{attr} vs Solar_MWh\")\n",
    "    plt.xlabel(attr)\n",
    "    plt.ylabel(\"Solar_MWh\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Erkennung von Zusammenhängen\n",
    "# Korrelationen zwischen Wetterattributen und Solar_MWh berechnen\n",
    "correlations = df_merged[weather_attributes + [\"Solar_MWh\"]].corr()[\"Solar_MWh\"].sort_values(ascending=False)\n",
    "print(\"Korrelationen mit Solar_MWh:\")\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Wolkenbedeckung weißt auf einen schwachen Zusammenhang zur Stromerzeugung, während die Sonneneinstrahlung wichtiger ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 3 Vorverarbeitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Behandlung von Ausreißern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verteilung von Solar_MWh\n",
    "sns.histplot(df_train[\"Solar_MWh\"], kde=True, bins=30)\n",
    "plt.title(\"Train - Verteilung von Solar_MWh\")\n",
    "plt.show()\n",
    "\n",
    "# Boxplot zur Erkennung von Ausreißern\n",
    "sns.boxplot(x=df_train[\"Solar_MWh\"])\n",
    "plt.title(\"Train - Boxplot von Solar_MWh\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: Es wäre für mich eine logische Entscheidung die Ausreißer 0 mitzunehmen ins Modell, weil diese möglicherweise echte Werte sind. Das liegt daran, das Nachts keine Stromproduktion stattfindet, sowie im Winter die Nacht länger andauert. <br>\n",
    "Zusätzlich ist es tagsüber deutlich inkonsistenter, da verschiedene Feature (Sonnenstrahlung, Wolkenbedeckung, Temperatur) Einfluss auf die die Stromproduktion nehmen.\n",
    "\n",
    "Zusammengefasst entscheide ich mich die Ausreißer nicht rauszunehmen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vorverarbeitung der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_merged.pop(\"Solar_MWh\")\n",
    "# Split in Trainings- und Testset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_merged, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vorverarbeitung = ColumnTransformer([\n",
    "    (\"O-H-Encoding\", OneHotEncoder(handle_unknown=\"ignore\"),[\"Weather Model\"]),\n",
    "    (\"nanTransform\", SimpleImputer(missing_values=np.nan, strategy=\"mean\"), [\"SolarDownwardRadiation\", \"CloudCover\", \"Temperature\" ]),\n",
    "    (\"Skalieren\", StandardScaler(), [\"Solar_capacity_mwp\", \"SolarDownwardRadiation\", \"CloudCover\", \"Temperature\"]),\n",
    "    (\"Entfernen von Spalten\", \"drop\", [\"dtm\", \"ref_datetime\", \"valid_time\", \"valid_datetime\"])\n",
    "], remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vorverarbeitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(vorverarbeitung.fit_transform(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 4 Generierung von neuen Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeitbasierte Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tageszeit\n",
    "df_merged[\"hour\"] = df_merged[\"dtm\"].dt.hour\n",
    "# Monat oder Saison\n",
    "df_merged[\"month\"] = df_merged[\"dtm\"].dt.month\n",
    "df_merged[\"season\"] = df_merged[\"month\"].apply(lambda x: (x % 12 + 3) // 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lag-Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[\"lag_1\"] = df_merged[\"Solar_MWh\"].shift(1)\n",
    "df_merged[\"lag_24\"] = df_merged[\"Solar_MWh\"].shift(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Features (Gleitender Durchschnitt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[\"rolling_mean_24\"] = df_merged[\"Solar_MWh\"].rolling(window=24).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaktion von Wetter und Zeit\n",
    "- temp_radiation_interaction: Produkt von Temperatur und Sonneneinstrahlung\n",
    "- cloud_hour_interaction: Produkt von CloudCover und Stunde des Tages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[\"temp_radiation_interaction\"] = (\n",
    "    df_merged[\"Temperature\"] * df_merged[\"SolarDownwardRadiation\"]\n",
    ")\n",
    "df_merged[\"cloud_hour_interaction\"] = (\n",
    "    df_merged[\"CloudCover\"] * df_merged[\"hour\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Merkmale\n",
    "Verhältnis von erzeugtem Strom zur verfügbaren Kapazität"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[\"solar_efficiency\"] = df_merged[\"Solar_MWh\"] / df_merged[\"Solar_capacity_mwp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation von Wetterattributen:\n",
    "- scaled_temperature: Skaliere Temperatur auf den Bereich [0, 1].\n",
    "- adjusted_radiation: Negative Werte auf 0 setzen (falls noch nicht gemacht)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df_merged[\"scaled_temperature\"] = scaler.fit_transform(\n",
    "    df_merged[[\"Temperature\"]]\n",
    ")\n",
    "df_merged[\"adjusted_radiation\"] = df_merged[\"SolarDownwardRadiation\"].clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 5 Modell trainieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline-Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_linear = Pipeline([(\"Vorverarbeitung\", vorverarbeitung), (\"Linear-Model Training\", LinearRegression())])\n",
    "pipe_linear.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe_linear.predict(X_test)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe_linear.predict(df_merged_test1)\n",
    "df_merged_test1[\"Solar_MWh_pred\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_test1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimierung von Ridge und Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter für Ridge und Lasso\n",
    "param_grid = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "ridge = GridSearchCV(Ridge(), param_grid, cv=5)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best alpha (Ridge):\", ridge.best_params_[\"alpha\"])\n",
    "print(\"Best RMSE (Ridge):\", -ridge.best_score_)\n",
    "\n",
    "# Lasso Regression\n",
    "lasso = GridSearchCV(Lasso(max_iter=10000), param_grid, scoring=\"neg_root_mean_squared_error\", cv=5)\n",
    "lasso.fit(X_train, y_train)\n",
    "print(\"Best alpha (Lasso):\", lasso.best_params_[\"alpha\"])\n",
    "print(\"Best RMSE (Lasso):\", -lasso.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell 2: Entscheidungsbaum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter für den Entscheidungsbaum\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5]\n",
    "}\n",
    "\n",
    "tree = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=5)\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"Best params (Decision Tree):\", tree.best_params_)\n",
    "print(\"Best RMSE (Decision Tree):\", -tree.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell 3: Ensemble-Modell (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter für Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [15, 20, 25],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5]\n",
    "}\n",
    "\n",
    "forest = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5)\n",
    "forest.fit(X_train, y_train)\n",
    "print(\"Best params (Random Forest):\", forest.best_params_)\n",
    "print(\"Best RMSE (Random Forest):\", -forest.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wichtigste Features bestimmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-Wichtigkeiten für Linear\n",
    "linear_coefficients = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns,\n",
    "    \"Importance\": pipe_linear.coef_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "print(\"Ridge Feature Importance:\\n\", linear_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-Wichtigkeiten für Ridge\n",
    "ridge_coefficients = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns,\n",
    "    \"Importance\": ridge.best_estimator_.coef_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "print(\"Ridge Feature Importance:\\n\", ridge_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-Wichtigkeiten für Random Forest\n",
    "forest_importance = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns,\n",
    "    #\"Importance\": forest.best_estimator_.feature_importances_\n",
    "})#.sort_values(by=\"Importance\", ascending=False)\n",
    "print(\"Random Forest Feature Importance:\\n\", forest_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_test1.to_pickle('test1.pkl')\n",
    "df_merged_test2.to_pickle('test2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zusammenfassung | Wenig Zellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 5 | 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful testing Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dtm</th>\n",
       "      <th>ref_datetime</th>\n",
       "      <th>Solar_capacity_mwp</th>\n",
       "      <th>Solar_MWh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [dtm, ref_datetime, Solar_capacity_mwp, Solar_MWh]\n",
       "Index: []"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train.Solar_MWh.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zusammengefassetes Modelltraining ohne zusätzliche Zellen\n",
    "# ? Kommentare sind gehighlighted mit der \"Better Comments\" Extension\n",
    "# * Bibliotheken laden und Daten einlesen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "df_test1 = pd.read_parquet(\"../Daten/energy_test1.parquet\")\n",
    "df_test2 = pd.read_parquet(\"../Daten/energy_test2.parquet\")\n",
    "df_train = pd.read_parquet(\"../Daten/energy_train.parquet\")\n",
    "df_forecasts = pd.read_parquet(\"../Daten/forecasts.parquet\")\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# ! Umgang von NaN-Werten \n",
    "# TODO Statt .dropna() eine Imputation einsetzen\n",
    "# ? Beim Forecast kann es ignoriert werden, da es im Transformer verwendet wird (aber erst im nach dem Merge)\n",
    "df_train = df_train.dropna()\n",
    "df_test1 = df_test1.dropna()\n",
    "df_test2 = df_test2.fillna(0)\n",
    "# df_forecasts = df_forecasts.dropna()\n",
    "# Spalte SolarDownwardRadiation\n",
    "df_forecasts[\"SolarDownwardRadiation\"] = df_forecasts[\"SolarDownwardRadiation\"].interpolate(method=\"linear\")\n",
    "# Spalte CloudCover\n",
    "df_forecasts[\"CloudCover\"] = df_forecasts[\"CloudCover\"].fillna(df_forecasts[\"CloudCover\"].median())\n",
    "# Spalte Temperature\n",
    "df_forecasts[\"Temperature\"] = df_forecasts[\"Temperature\"].interpolate(method=\"linear\")\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# ! Umgang von Negativen Werten\n",
    "# TODO Kann auch ignoriert werden\n",
    "# df_forecasts[\"SolarDownwardRadiation\"] = df_forecasts[\"SolarDownwardRadiation\"].clip(lower=0)\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# ! Merge von Test-/Trainingsdaten mit Forecast-Dataset\n",
    "# TODO Konvertiere `ref_datetime` und `dtm` zu datetime-Objekten\n",
    "df_train[\"ref_datetime\"] = pd.to_datetime(df_train[\"ref_datetime\"])\n",
    "df_test1[\"ref_datetime\"] = pd.to_datetime(df_test1[\"ref_datetime\"])\n",
    "df_test2[\"ref_datetime\"] = pd.to_datetime(df_test2[\"ref_datetime\"])\n",
    "df_train[\"dtm\"] = pd.to_datetime(df_train[\"dtm\"])\n",
    "df_test1[\"dtm\"] = pd.to_datetime(df_test1[\"dtm\"])\n",
    "df_test2[\"dtm\"] = pd.to_datetime(df_test2[\"dtm\"])\n",
    "\n",
    "# Gültigen Zeitpunkt für Wettervorhersagen berechnen\n",
    "df_forecasts[\"valid_datetime\"] = df_forecasts[\"ref_datetime\"] + pd.to_timedelta(df_forecasts[\"valid_time\"], unit=\"h\")\n",
    "\n",
    "df_merged = pd.merge(\n",
    "    df_train,\n",
    "    df_forecasts,\n",
    "    left_on=[\"dtm\", \"ref_datetime\"],\n",
    "    right_on=[\"valid_datetime\", \"ref_datetime\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "df_merged_test1 = pd.merge(\n",
    "    df_test1,\n",
    "    df_forecasts,\n",
    "    left_on=[\"dtm\", \"ref_datetime\"],\n",
    "    right_on=[\"valid_datetime\", \"ref_datetime\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "df_merged_test2 = pd.merge(\n",
    "    df_test2,\n",
    "    df_forecasts,\n",
    "    left_on=[\"dtm\", \"ref_datetime\"],\n",
    "    right_on=[\"valid_datetime\", \"ref_datetime\"],\n",
    "    how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vom fehlerhaften Merge\n",
    "# // df_merged = df_merged.rename(columns={\"ref_datetime_x\": \"date_train\", \"ref_datetime_y\": \"date_weather\"})\n",
    "# // df_merged_test1 = df_merged_test1.rename(columns={\"ref_datetime_x\": \"date_train\", \"ref_datetime_y\": \"date_weather\"})\n",
    "# // df_merged_test2 = df_merged_test2.rename(columns={\"ref_datetime_x\": \"date_train\", \"ref_datetime_y\": \"date_weather\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering und Vorverarbeitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zeitbasiert Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion für zyklische Transformation | Abruf im ColumnTransformer\n",
    "def cyclical_features(X):\n",
    "    X = X.copy()\n",
    "    X[\"hour_sin\"] = np.sin(2 * np.pi * X[\"hour\"] / 24)\n",
    "    X[\"hour_cos\"] = np.cos(2 * np.pi * X[\"hour\"] / 24)\n",
    "    return X[[\"hour_sin\", \"hour_cos\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methode zur Bestimmung des Zeitraums\n",
    "def assign_time_period(hour):\n",
    "    if 5 <= hour < 9:\n",
    "        return 'Morgen'\n",
    "    elif 9 <= hour < 12:\n",
    "        return 'Vormittag'\n",
    "    elif 12 <= hour < 15:\n",
    "        return 'Mittag'\n",
    "    elif 15 <= hour < 18:\n",
    "        return 'Nachmittag'\n",
    "    elif 18 <= hour < 22:\n",
    "        return 'Abend'\n",
    "    else:\n",
    "        return 'Nacht'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ! Feature Engineering\n",
    "merged_list = [df_merged, df_merged_test1, df_merged_test2]\n",
    "\n",
    "for add_time in merged_list:  \n",
    "    # Tageszeit\n",
    "    add_time[\"hour\"] = add_time[\"dtm\"].dt.hour\n",
    "    add_time[\"time_period\"] = add_time[\"hour\"].apply(assign_time_period)\n",
    "    # add_time[\"day_of_week\"] = add_time[\"dtm\"].dt.day_of_week\n",
    "    # Monat oder Saison\n",
    "    add_time[\"month\"] = add_time[\"dtm\"].dt.month\n",
    "    add_time[\"season\"] = add_time[\"month\"].apply(lambda x: (x % 12 + 3) // 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wetterbasierte Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_temperature_level(temperature):\n",
    "    if temperature < 0:\n",
    "        return 'Sehr kalt'\n",
    "    elif 0 <= temperature < 10:\n",
    "        return 'Kalt'\n",
    "    elif 10 <= temperature < 15:\n",
    "        return 'Kühl'\n",
    "    elif 15 <= temperature < 20:\n",
    "        return 'Mild'\n",
    "    elif 20 <= temperature < 25:\n",
    "        return 'Warm'\n",
    "    elif 25 <= temperature < 30:\n",
    "        return 'Sehr warm'\n",
    "    elif 30 <= temperature < 35:\n",
    "        return 'Heiß'\n",
    "    else:\n",
    "        return 'Sehr heiß'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ! Feature Engineering\n",
    "merged_list = [df_merged, df_merged_test1, df_merged_test2]\n",
    "\n",
    "for add_weather in merged_list:  \n",
    "    # Interaktion zwischen Sonneneinstrahlung und Temperatur: Hohe Temperaturen können die Effizienz von Solaranlagen reduzieren, trotz hoher Sonneneinstrahlung\n",
    "    add_weather[\"Sun_CloudCover\"] = add_weather[\"Temperature\"] * add_weather[\"SolarDownwardRadiation\"]\n",
    "    # Bewölkerungsdynamik\n",
    "    add_weather[\"CloudCover_change\"] = add_weather[\"CloudCover\"].diff()\n",
    "    add_weather[\"Temperature_Level\"] = add_weather[\"Temperature\"].apply(assign_temperature_level)\n",
    "    \n",
    "df_merged = df_merged.fillna(0)\n",
    "df_merged_test1 = df_merged_test1.fillna(0)\n",
    "df_merged_test2 = df_merged_test2.fillna(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Historische Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ! Feature Engineering\n",
    "merged_list = [df_merged, df_merged_test1, df_merged_test2]\n",
    "\n",
    "for add_weather in merged_list:  \n",
    "    # Rolling Average für Sonneneinstrahlung (z. B. über die letzten 3 Stunden)\n",
    "    # add_weather['AvgSolarRadiation_last_3h'] = add_weather['SolarDownwardRadiation'].rolling(window=3).mean()\n",
    "    # # Lag-Feature: Sonneneinstrahlung der letzten Stunde\n",
    "    add_weather['SolarRadiation_lag_1h'] = add_weather['SolarDownwardRadiation'].shift(1)\n",
    "    # # Lag-Feature: Temperatur der letzten Stunde\n",
    "    # add_weather['Temperature_lag_1h'] = add_weather['Temperature'].shift(1)\n",
    "    \n",
    "df_merged = df_merged.fillna(0)\n",
    "df_merged_test1 = df_merged_test1.fillna(0)\n",
    "df_merged_test2 = df_merged_test2.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Vorverarbeitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# * Daten aufteilen\n",
    "y = df_merged.pop(\"Solar_MWh\")\n",
    "# * Split in Trainings- und Testset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_merged, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ! Vorverarbeitung: Transformer\n",
    "vorverarbeitung = ColumnTransformer([\n",
    "    (\"O-H-Encoding\", OneHotEncoder(handle_unknown=\"ignore\"),[\"Weather Model\", \"time_period\", \"season\", \"Temperature_Level\"]),\n",
    "    (\"Zyklisch_hour\", FunctionTransformer(cyclical_features), [\"hour\"]),\n",
    "    #(\"nanTransform\", SimpleImputer(missing_values=np.nan, strategy=\"mean\"), [\"SolarDownwardRadiation\", \"CloudCover\", \"Temperature\"]),\n",
    "    (\"Skalieren\", StandardScaler(), [\n",
    "        \"SolarDownwardRadiation\", \"Sun_CloudCover\", \"CloudCover_change\", \"SolarRadiation_lag_1h\"\n",
    "        ]),\n",
    "    (\"drop_columns\", 'drop', [\"dtm\", \"valid_time\", \"valid_datetime\", \"Solar_capacity_mwp\", \"ref_datetime\", \"hour\", \"month\", \"CloudCover\", \"Temperature\"])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN-Werte: dtm                       0\n",
      "ref_datetime              0\n",
      "Solar_capacity_mwp        0\n",
      "valid_time                0\n",
      "SolarDownwardRadiation    0\n",
      "CloudCover                0\n",
      "Temperature               0\n",
      "Weather Model             0\n",
      "valid_datetime            0\n",
      "hour                      0\n",
      "time_period               0\n",
      "month                     0\n",
      "season                    0\n",
      "Sun_CloudCover            0\n",
      "CloudCover_change         0\n",
      "Temperature_Level         0\n",
      "SolarRadiation_lag_1h     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"NaN-Werte:\", X_train.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lineares Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter: {'fit_intercept': True}\n",
      "RMSE: 56.87735055281777\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# !Pipeline: Lineares Modell\n",
    "# * Hyperparametersuche\n",
    "param_grid = {\"fit_intercept\": [True, False]}\n",
    "\n",
    "pipe_linear = Pipeline([(\"Vorverarbeitung\", vorverarbeitung),\n",
    "                        (\"gs_linear\", GridSearchCV(LinearRegression(), param_grid, cv=5))\n",
    "                         ])\n",
    "\n",
    "# Trainiere Lineares Modell\n",
    "pipe_linear.fit(X_train, y_train)\n",
    "\n",
    "# Extrahiere das beste Modell und die besten Parameter\n",
    "best_params = pipe_linear.named_steps[\"gs_linear\"].best_params_\n",
    "print(\"Beste Parameter:\", best_params)\n",
    "\n",
    "# RMSE\n",
    "y_pred = pipe_linear.predict(X_test)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "# Vorhersage für Testdaten 1 und 2\n",
    "y_pred = pipe_linear.predict(df_merged_test1)\n",
    "df_merged_test1[\"Solar_MWh_pred\"] = y_pred\n",
    "y_pred = pipe_linear.predict(df_merged_test2)\n",
    "df_merged_test2[\"Solar_MWh_pred\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(56.87735055281777)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_rmse = rmse\n",
    "old_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Transformer Zyklisch_hour (type FunctionTransformer) does not provide get_feature_names_out.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m best_linear_model \u001b[38;5;241m=\u001b[39m pipe_linear\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs_linear\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Abrufe der Feature-Namen nach Vorverarbeitung\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m pipe_linear\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVorverarbeitung\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Erstelle einen DataFrame mit den Koeffizienten\u001b[39;00m\n\u001b[0;32m      8\u001b[0m coefficients \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m\"\u001b[39m: feature_names,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoefficient\u001b[39m\u001b[38;5;124m\"\u001b[39m: best_linear_model\u001b[38;5;241m.\u001b[39mcoef_\n\u001b[0;32m     11\u001b[0m })\n",
      "File \u001b[1;32mc:\\Users\\achim\\anaconda3\\envs\\dsc4\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:630\u001b[0m, in \u001b[0;36mColumnTransformer.get_feature_names_out\u001b[1;34m(self, input_features)\u001b[0m\n\u001b[0;32m    623\u001b[0m transformer_with_feature_names_out \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, trans, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(\n\u001b[0;32m    625\u001b[0m     fitted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    626\u001b[0m     column_as_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    627\u001b[0m     skip_empty_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    628\u001b[0m     skip_drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    629\u001b[0m ):\n\u001b[1;32m--> 630\u001b[0m     feature_names_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_feature_name_out_for_transformer(\n\u001b[0;32m    631\u001b[0m         name, trans, input_features\n\u001b[0;32m    632\u001b[0m     )\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feature_names_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\achim\\anaconda3\\envs\\dsc4\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:593\u001b[0m, in \u001b[0;36mColumnTransformer._get_feature_name_out_for_transformer\u001b[1;34m(self, name, trans, feature_names_in)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;66;03m# An actual transformer\u001b[39;00m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trans, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_feature_names_out\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    594\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(trans)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) does \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    595\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot provide get_feature_names_out.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    596\u001b[0m     )\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trans\u001b[38;5;241m.\u001b[39mget_feature_names_out(names)\n",
      "\u001b[1;31mAttributeError\u001b[0m: Transformer Zyklisch_hour (type FunctionTransformer) does not provide get_feature_names_out."
     ]
    }
   ],
   "source": [
    "\n",
    "# Extrahiere den besten Linearen Regressor aus GridSearchCV\n",
    "best_linear_model = pipe_linear.named_steps[\"gs_linear\"].best_estimator_\n",
    "\n",
    "# Abrufe der Feature-Namen nach Vorverarbeitung\n",
    "feature_names = pipe_linear.named_steps[\"Vorverarbeitung\"].get_feature_names_out()\n",
    "\n",
    "# Erstelle einen DataFrame mit den Koeffizienten\n",
    "coefficients = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Coefficient\": best_linear_model.coef_\n",
    "})\n",
    "\n",
    "coefficients[\"Feature_Importance\"] = coefficients[\"Coefficient\"].abs().round(1)\n",
    "coefficients = coefficients.sort_values(by=\"Feature_Importance\", ascending=False)\n",
    "\n",
    "# Ausgabe der wichtigsten Features\n",
    "coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge und Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter für Ridge und Lasso\n",
    "param_grid = {'alpha': [0.01, 0.05, 0.1, 0.5, 1, 1.5, 2, 10, 20, 40, 80]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter: {'alpha': 1}\n",
      "RMSE: 58.4996850092601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achim\\AppData\\Local\\Temp\\ipykernel_21264\\3921426539.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_merged_test1[\"Solar_MWh_pred\"] = y_pred\n",
      "C:\\Users\\achim\\AppData\\Local\\Temp\\ipykernel_21264\\3921426539.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_merged_test2[\"Solar_MWh_pred\"] = y_pred\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# !Pipeline: Ridge Modell\n",
    "pipe_ridge = Pipeline([(\"Vorverarbeitung\", vorverarbeitung),\n",
    "                       (\"gs_ridge\", GridSearchCV(Ridge(), param_grid, cv=5))\n",
    "                       ])\n",
    "pipe_ridge.fit(X_train, y_train)\n",
    "\n",
    "# Extrahiere das beste Modell und die besten Parameter\n",
    "best_params = pipe_ridge.named_steps[\"gs_ridge\"].best_params_\n",
    "print(\"Beste Parameter:\", best_params)\n",
    "\n",
    "y_pred = pipe_ridge.predict(X_test)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "y_pred = pipe_ridge.predict(df_merged_test1)\n",
    "df_merged_test1[\"Solar_MWh_pred\"] = y_pred\n",
    "y_pred = pipe_ridge.predict(df_merged_test2)\n",
    "df_merged_test2[\"Solar_MWh_pred\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(58.4996850092601)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_rmse = rmse\n",
    "old_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Feature_Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Skalieren__SolarDownwardRadiation</td>\n",
       "      <td>103.291188</td>\n",
       "      <td>103.291188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Skalieren__SolarRadiation_lag_1h</td>\n",
       "      <td>65.101986</td>\n",
       "      <td>65.101986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>O-H-Encoding__time_period_Vormittag</td>\n",
       "      <td>46.575643</td>\n",
       "      <td>46.575643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>O-H-Encoding__time_period_Nachmittag</td>\n",
       "      <td>-31.374148</td>\n",
       "      <td>31.374148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O-H-Encoding__time_period_Mittag</td>\n",
       "      <td>22.352317</td>\n",
       "      <td>22.352317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O-H-Encoding__time_period_Abend</td>\n",
       "      <td>-16.905081</td>\n",
       "      <td>16.905081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>O-H-Encoding__time_period_Nacht</td>\n",
       "      <td>-10.621326</td>\n",
       "      <td>10.621326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O-H-Encoding__time_period_Morgen</td>\n",
       "      <td>-10.027404</td>\n",
       "      <td>10.027404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>O-H-Encoding__season_3</td>\n",
       "      <td>-9.878982</td>\n",
       "      <td>9.878982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>O-H-Encoding__season_4</td>\n",
       "      <td>5.189995</td>\n",
       "      <td>5.189995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>O-H-Encoding__season_1</td>\n",
       "      <td>3.169264</td>\n",
       "      <td>3.169264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Skalieren__CloudCover_change</td>\n",
       "      <td>2.046379</td>\n",
       "      <td>2.046379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O-H-Encoding__Weather Model_NCEP GFS</td>\n",
       "      <td>-1.966865</td>\n",
       "      <td>1.966865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O-H-Encoding__Weather Model_DWD ICON</td>\n",
       "      <td>1.966865</td>\n",
       "      <td>1.966865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>O-H-Encoding__season_2</td>\n",
       "      <td>1.519724</td>\n",
       "      <td>1.519724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Skalieren__Sun_CloudCover</td>\n",
       "      <td>-1.298798</td>\n",
       "      <td>1.298798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Feature  Coefficient  Feature_Importance\n",
       "12     Skalieren__SolarDownwardRadiation   103.291188          103.291188\n",
       "15      Skalieren__SolarRadiation_lag_1h    65.101986           65.101986\n",
       "7    O-H-Encoding__time_period_Vormittag    46.575643           46.575643\n",
       "5   O-H-Encoding__time_period_Nachmittag   -31.374148           31.374148\n",
       "3       O-H-Encoding__time_period_Mittag    22.352317           22.352317\n",
       "2        O-H-Encoding__time_period_Abend   -16.905081           16.905081\n",
       "6        O-H-Encoding__time_period_Nacht   -10.621326           10.621326\n",
       "4       O-H-Encoding__time_period_Morgen   -10.027404           10.027404\n",
       "10                O-H-Encoding__season_3    -9.878982            9.878982\n",
       "11                O-H-Encoding__season_4     5.189995            5.189995\n",
       "8                 O-H-Encoding__season_1     3.169264            3.169264\n",
       "14          Skalieren__CloudCover_change     2.046379            2.046379\n",
       "1   O-H-Encoding__Weather Model_NCEP GFS    -1.966865            1.966865\n",
       "0   O-H-Encoding__Weather Model_DWD ICON     1.966865            1.966865\n",
       "9                 O-H-Encoding__season_2     1.519724            1.519724\n",
       "13             Skalieren__Sun_CloudCover    -1.298798            1.298798"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extrahiere den besten Linearen Regressor aus GridSearchCV\n",
    "best_ridge_model = pipe_ridge.named_steps[\"gs_ridge\"].best_estimator_\n",
    "\n",
    "# Abrufe der Feature-Namen nach Vorverarbeitung\n",
    "feature_names = pipe_ridge.named_steps[\"Vorverarbeitung\"].get_feature_names_out()\n",
    "\n",
    "# Erstelle einen DataFrame mit den Koeffizienten\n",
    "coefficients = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Coefficient\": best_ridge_model.coef_\n",
    "})\n",
    "\n",
    "coefficients[\"Feature_Importance\"] = coefficients[\"Coefficient\"].abs()\n",
    "coefficients = coefficients.sort_values(by=\"Feature_Importance\", ascending=False)\n",
    "\n",
    "# Ausgabe der wichtigsten Features\n",
    "coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter: {'alpha': 0.01}\n",
      "RMSE: 58.50693605461164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achim\\AppData\\Local\\Temp\\ipykernel_21264\\466459414.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_merged_test1[\"Solar_MWh_pred\"] = y_pred\n",
      "C:\\Users\\achim\\AppData\\Local\\Temp\\ipykernel_21264\\466459414.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_merged_test2[\"Solar_MWh_pred\"] = y_pred\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# !Pipeline: Lasso Modell\n",
    "pipe_lasso = Pipeline([(\"Vorverarbeitung\", vorverarbeitung),\n",
    "                       (\"gs_lasso\", GridSearchCV(Lasso(), param_grid, cv=5))\n",
    "                       ])\n",
    "pipe_lasso.fit(X_train, y_train)\n",
    "\n",
    "# Extrahiere das beste Modell und die besten Parameter\n",
    "best_params = pipe_lasso.named_steps[\"gs_lasso\"].best_params_\n",
    "print(\"Beste Parameter:\", best_params)\n",
    "\n",
    "y_pred = pipe_lasso.predict(X_test)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "y_pred = pipe_lasso.predict(df_merged_test1)\n",
    "df_merged_test1[\"Solar_MWh_pred\"] = y_pred\n",
    "y_pred = pipe_lasso.predict(df_merged_test2)\n",
    "df_merged_test2[\"Solar_MWh_pred\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(58.50693605461164)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_rmse = rmse\n",
    "old_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Feature_Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Skalieren__SolarDownwardRadiation</td>\n",
       "      <td>1.208729e+02</td>\n",
       "      <td>1.208729e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Skalieren__SolarRadiation_lag_1h</td>\n",
       "      <td>7.825981e+01</td>\n",
       "      <td>7.825981e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>O-H-Encoding__time_period_Vormittag</td>\n",
       "      <td>5.539818e+01</td>\n",
       "      <td>5.539818e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O-H-Encoding__time_period_Mittag</td>\n",
       "      <td>3.590930e+01</td>\n",
       "      <td>3.590930e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Skalieren__AvgSolarRadiation_last_3h</td>\n",
       "      <td>-3.126565e+01</td>\n",
       "      <td>3.126565e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>O-H-Encoding__time_period_Nachmittag</td>\n",
       "      <td>-1.668769e+01</td>\n",
       "      <td>1.668769e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>O-H-Encoding__season_3</td>\n",
       "      <td>-1.170344e+01</td>\n",
       "      <td>1.170344e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O-H-Encoding__time_period_Abend</td>\n",
       "      <td>-5.448076e+00</td>\n",
       "      <td>5.448076e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O-H-Encoding__Weather Model_DWD ICON</td>\n",
       "      <td>3.087895e+00</td>\n",
       "      <td>3.087895e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>O-H-Encoding__season_4</td>\n",
       "      <td>2.677540e+00</td>\n",
       "      <td>2.677540e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Skalieren__CloudCover_change</td>\n",
       "      <td>1.989835e+00</td>\n",
       "      <td>1.989835e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>O-H-Encoding__season_1</td>\n",
       "      <td>9.560960e-01</td>\n",
       "      <td>9.560960e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O-H-Encoding__time_period_Morgen</td>\n",
       "      <td>-8.065198e-01</td>\n",
       "      <td>8.065198e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Skalieren__Sun_CloudCover</td>\n",
       "      <td>-7.296114e-01</td>\n",
       "      <td>7.296114e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>O-H-Encoding__season_2</td>\n",
       "      <td>-1.819113e-01</td>\n",
       "      <td>1.819113e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O-H-Encoding__Weather Model_NCEP GFS</td>\n",
       "      <td>-1.437128e-16</td>\n",
       "      <td>1.437128e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>O-H-Encoding__time_period_Nacht</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Feature   Coefficient  Feature_Importance\n",
       "12     Skalieren__SolarDownwardRadiation  1.208729e+02        1.208729e+02\n",
       "15      Skalieren__SolarRadiation_lag_1h  7.825981e+01        7.825981e+01\n",
       "7    O-H-Encoding__time_period_Vormittag  5.539818e+01        5.539818e+01\n",
       "3       O-H-Encoding__time_period_Mittag  3.590930e+01        3.590930e+01\n",
       "16  Skalieren__AvgSolarRadiation_last_3h -3.126565e+01        3.126565e+01\n",
       "5   O-H-Encoding__time_period_Nachmittag -1.668769e+01        1.668769e+01\n",
       "10                O-H-Encoding__season_3 -1.170344e+01        1.170344e+01\n",
       "2        O-H-Encoding__time_period_Abend -5.448076e+00        5.448076e+00\n",
       "0   O-H-Encoding__Weather Model_DWD ICON  3.087895e+00        3.087895e+00\n",
       "11                O-H-Encoding__season_4  2.677540e+00        2.677540e+00\n",
       "14          Skalieren__CloudCover_change  1.989835e+00        1.989835e+00\n",
       "8                 O-H-Encoding__season_1  9.560960e-01        9.560960e-01\n",
       "4       O-H-Encoding__time_period_Morgen -8.065198e-01        8.065198e-01\n",
       "13             Skalieren__Sun_CloudCover -7.296114e-01        7.296114e-01\n",
       "9                 O-H-Encoding__season_2 -1.819113e-01        1.819113e-01\n",
       "1   O-H-Encoding__Weather Model_NCEP GFS -1.437128e-16        1.437128e-16\n",
       "6        O-H-Encoding__time_period_Nacht  0.000000e+00        0.000000e+00"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extrahiere den besten Linearen Regressor aus GridSearchCV\n",
    "best_lasso_model = pipe_lasso.named_steps[\"gs_lasso\"].best_estimator_\n",
    "\n",
    "# Abrufe der Feature-Namen nach Vorverarbeitung\n",
    "feature_names = pipe_lasso.named_steps[\"Vorverarbeitung\"].get_feature_names_out()\n",
    "\n",
    "# Erstelle einen DataFrame mit den Koeffizienten\n",
    "coefficients = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Coefficient\": best_lasso_model.coef_\n",
    "})\n",
    "\n",
    "coefficients[\"Feature_Importance\"] = coefficients[\"Coefficient\"].abs()\n",
    "coefficients = coefficients.sort_values(by=\"Feature_Importance\", ascending=False)\n",
    "\n",
    "# Ausgabe der wichtigsten Features\n",
    "coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter für den Entscheidungsbaum\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, 15],\n",
    "    'min_samples_split': [2, 4, 5, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter: {'max_depth': 10, 'min_samples_leaf': 5, 'min_samples_split': 8}\n",
      "RMSE: 53.25668774730165\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# !Pipeline: DecisionTree Modell\n",
    "pipe_tree = Pipeline([(\"Vorverarbeitung\", vorverarbeitung),\n",
    "                       (\"gs_tree\", GridSearchCV(DecisionTreeRegressor(), param_grid, cv=5))\n",
    "                       ])\n",
    "pipe_tree.fit(X_train, y_train)\n",
    "\n",
    "# Extrahiere das beste Modell und die besten Parameter\n",
    "best_params = pipe_tree.named_steps[\"gs_tree\"].best_params_\n",
    "print(\"Beste Parameter:\", best_params)\n",
    "\n",
    "y_pred = pipe_tree.predict(X_test)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "y_pred = pipe_tree.predict(df_merged_test1)\n",
    "df_merged_test1[\"Solar_MWh_pred\"] = y_pred\n",
    "y_pred = pipe_tree.predict(df_merged_test2)\n",
    "df_merged_test2[\"Solar_MWh_pred\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(53.25668774730165)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_rmse = rmse\n",
    "old_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Skalieren__SolarDownwardRadiation</td>\n",
       "      <td>0.819577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Skalieren__AvgSolarRadiation_last_3h</td>\n",
       "      <td>0.117158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Skalieren__SolarRadiation_lag_1h</td>\n",
       "      <td>0.033221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>O-H-Encoding__time_period_Nachmittag</td>\n",
       "      <td>0.006381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>O-H-Encoding__time_period_Vormittag</td>\n",
       "      <td>0.005624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Skalieren__Sun_CloudCover</td>\n",
       "      <td>0.004533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Skalieren__CloudCover_change</td>\n",
       "      <td>0.003790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O-H-Encoding__Weather Model_NCEP GFS</td>\n",
       "      <td>0.003241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O-H-Encoding__time_period_Mittag</td>\n",
       "      <td>0.001616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>O-H-Encoding__season_1</td>\n",
       "      <td>0.001535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>O-H-Encoding__season_3</td>\n",
       "      <td>0.001166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O-H-Encoding__time_period_Morgen</td>\n",
       "      <td>0.000693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O-H-Encoding__time_period_Abend</td>\n",
       "      <td>0.000456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>O-H-Encoding__season_4</td>\n",
       "      <td>0.000432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O-H-Encoding__Weather Model_DWD ICON</td>\n",
       "      <td>0.000293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>O-H-Encoding__season_2</td>\n",
       "      <td>0.000276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>O-H-Encoding__time_period_Nacht</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Feature  Importance\n",
       "12     Skalieren__SolarDownwardRadiation    0.819577\n",
       "16  Skalieren__AvgSolarRadiation_last_3h    0.117158\n",
       "15      Skalieren__SolarRadiation_lag_1h    0.033221\n",
       "5   O-H-Encoding__time_period_Nachmittag    0.006381\n",
       "7    O-H-Encoding__time_period_Vormittag    0.005624\n",
       "13             Skalieren__Sun_CloudCover    0.004533\n",
       "14          Skalieren__CloudCover_change    0.003790\n",
       "1   O-H-Encoding__Weather Model_NCEP GFS    0.003241\n",
       "3       O-H-Encoding__time_period_Mittag    0.001616\n",
       "8                 O-H-Encoding__season_1    0.001535\n",
       "10                O-H-Encoding__season_3    0.001166\n",
       "4       O-H-Encoding__time_period_Morgen    0.000693\n",
       "2        O-H-Encoding__time_period_Abend    0.000456\n",
       "11                O-H-Encoding__season_4    0.000432\n",
       "0   O-H-Encoding__Weather Model_DWD ICON    0.000293\n",
       "9                 O-H-Encoding__season_2    0.000276\n",
       "6        O-H-Encoding__time_period_Nacht    0.000007"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extrahiere den besten DecisionTreeRegressor aus GridSearchCV\n",
    "best_tree_model = pipe_tree.named_steps[\"gs_tree\"].best_estimator_\n",
    "\n",
    "# Abrufen der Feature-Namen nach Vorverarbeitung\n",
    "feature_names = pipe_tree.named_steps[\"Vorverarbeitung\"].get_feature_names_out()\n",
    "\n",
    "# Erstelle einen DataFrame mit den Feature Importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": best_tree_model.feature_importances_\n",
    "})\n",
    "\n",
    "# Sortiere die Features nach ihrer Importance (absolut, um die wichtigsten Features zuerst zu sehen)\n",
    "feature_importances = feature_importances.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Ausgabe der Feature Importances\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble-Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 30}\n",
      "RMSE: 49.583684212439465\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter für Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [15, 20, 25, 30],\n",
    "    'max_depth': [8, 10, 13],\n",
    "    'min_samples_split': [2, 5, 10, 13],\n",
    "    'min_samples_leaf': [1, 2, 5]\n",
    "}\n",
    "# !Pipeline: Random Forest Modell\n",
    "pipe_rand_frst = Pipeline([(\"Vorverarbeitung\", vorverarbeitung),\n",
    "                       (\"gs_rand_frst\", GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5, n_jobs=-1))\n",
    "                       ])\n",
    "pipe_rand_frst.fit(X_train, y_train)\n",
    "\n",
    "# Extrahiere das beste Modell und die besten Parameter\n",
    "best_params = pipe_rand_frst.named_steps[\"gs_rand_frst\"].best_params_\n",
    "print(\"Beste Parameter:\", best_params)\n",
    "\n",
    "y_pred = pipe_rand_frst.predict(X_test)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "y_pred = pipe_rand_frst.predict(df_merged_test1)\n",
    "df_merged_test1[\"Solar_MWh_pred\"] = y_pred\n",
    "y_pred = pipe_rand_frst.predict(df_merged_test2)\n",
    "df_merged_test2[\"Solar_MWh_pred\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(49.583684212439465)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_rmse = rmse\n",
    "old_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Skalieren__SolarDownwardRadiation</td>\n",
       "      <td>0.812659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Skalieren__AvgSolarRadiation_last_3h</td>\n",
       "      <td>0.120524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Skalieren__SolarRadiation_lag_1h</td>\n",
       "      <td>0.034452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>O-H-Encoding__time_period_Nachmittag</td>\n",
       "      <td>0.006365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>O-H-Encoding__time_period_Vormittag</td>\n",
       "      <td>0.006199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Skalieren__Sun_CloudCover</td>\n",
       "      <td>0.005338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Skalieren__CloudCover_change</td>\n",
       "      <td>0.004979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O-H-Encoding__time_period_Mittag</td>\n",
       "      <td>0.001910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O-H-Encoding__Weather Model_NCEP GFS</td>\n",
       "      <td>0.001623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>O-H-Encoding__season_3</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>O-H-Encoding__season_1</td>\n",
       "      <td>0.001358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O-H-Encoding__Weather Model_DWD ICON</td>\n",
       "      <td>0.001321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O-H-Encoding__time_period_Morgen</td>\n",
       "      <td>0.000505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O-H-Encoding__time_period_Abend</td>\n",
       "      <td>0.000488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>O-H-Encoding__season_2</td>\n",
       "      <td>0.000399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>O-H-Encoding__season_4</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>O-H-Encoding__time_period_Nacht</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Feature  Importance\n",
       "12     Skalieren__SolarDownwardRadiation    0.812659\n",
       "16  Skalieren__AvgSolarRadiation_last_3h    0.120524\n",
       "15      Skalieren__SolarRadiation_lag_1h    0.034452\n",
       "5   O-H-Encoding__time_period_Nachmittag    0.006365\n",
       "7    O-H-Encoding__time_period_Vormittag    0.006199\n",
       "13             Skalieren__Sun_CloudCover    0.005338\n",
       "14          Skalieren__CloudCover_change    0.004979\n",
       "3       O-H-Encoding__time_period_Mittag    0.001910\n",
       "1   O-H-Encoding__Weather Model_NCEP GFS    0.001623\n",
       "10                O-H-Encoding__season_3    0.001500\n",
       "8                 O-H-Encoding__season_1    0.001358\n",
       "0   O-H-Encoding__Weather Model_DWD ICON    0.001321\n",
       "4       O-H-Encoding__time_period_Morgen    0.000505\n",
       "2        O-H-Encoding__time_period_Abend    0.000488\n",
       "9                 O-H-Encoding__season_2    0.000399\n",
       "11                O-H-Encoding__season_4    0.000375\n",
       "6        O-H-Encoding__time_period_Nacht    0.000004"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extrahiere den besten DecisionTreeRegressor aus GridSearchCV\n",
    "best_randtree_model = pipe_rand_frst.named_steps[\"gs_rand_frst\"].best_estimator_\n",
    "\n",
    "# Abrufen der Feature-Namen nach Vorverarbeitung\n",
    "feature_names = pipe_rand_frst.named_steps[\"Vorverarbeitung\"].get_feature_names_out()\n",
    "\n",
    "# Erstelle einen DataFrame mit den Feature Importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": best_randtree_model.feature_importances_\n",
    "})\n",
    "\n",
    "# Sortiere die Features nach ihrer Importance (absolut, um die wichtigsten Features zuerst zu sehen)\n",
    "feature_importances = feature_importances.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Ausgabe der Feature Importances\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# !Pipeline: Gradient Boosting Modell\u001b[39;00m\n\u001b[0;32m      9\u001b[0m pipe_grad_boost \u001b[38;5;241m=\u001b[39m Pipeline([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVorverarbeitung\u001b[39m\u001b[38;5;124m\"\u001b[39m, vorverarbeitung),\n\u001b[0;32m     10\u001b[0m                        (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs_grad_boost\u001b[39m\u001b[38;5;124m\"\u001b[39m, GridSearchCV(GradientBoostingRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m), param_grid, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     11\u001b[0m                        ])\n\u001b[1;32m---> 12\u001b[0m pipe_grad_boost\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Extrahiere das beste Modell und die besten Parameter\u001b[39;00m\n\u001b[0;32m     15\u001b[0m best_params \u001b[38;5;241m=\u001b[39m pipe_grad_boost\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs_grad_boost\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[1;32mc:\\Users\\achim\\anaconda3\\envs\\dsc4\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\achim\\anaconda3\\envs\\dsc4\\Lib\\site-packages\\sklearn\\pipeline.py:476\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    475\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 476\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\achim\\anaconda3\\envs\\dsc4\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\achim\\anaconda3\\envs\\dsc4\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:968\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    962\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    963\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    964\u001b[0m     )\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 968\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    972\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\achim\\anaconda3\\envs\\dsc4\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1543\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1541\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1543\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\achim\\anaconda3\\envs\\dsc4\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:914\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    909\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    910\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    911\u001b[0m         )\n\u001b[0;32m    912\u001b[0m     )\n\u001b[1;32m--> 914\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    915\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    916\u001b[0m         clone(base_estimator),\n\u001b[0;32m    917\u001b[0m         X,\n\u001b[0;32m    918\u001b[0m         y,\n\u001b[0;32m    919\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    920\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    921\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    922\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    923\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    924\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    925\u001b[0m     )\n\u001b[0;32m    926\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    927\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    928\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    929\u001b[0m     )\n\u001b[0;32m    930\u001b[0m )\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    934\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    935\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\achim\\anaconda3\\envs\\dsc4\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\achim\\anaconda3\\envs\\dsc4\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\achim\\anaconda3\\envs\\dsc4\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\achim\\anaconda3\\envs\\dsc4\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameter für Gradient Boosting\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.05, 0.1, 0.15],\n",
    "    \"n_estimators\": [75, 100, 125],\n",
    "    \"max_depth\": [6, 7, 8, 9],\n",
    "    \"max_features\": [None, \"sqrt\", \"log2\"]\n",
    "}\n",
    "# !Pipeline: Gradient Boosting Modell\n",
    "pipe_grad_boost = Pipeline([(\"Vorverarbeitung\", vorverarbeitung),\n",
    "                       (\"gs_grad_boost\", GridSearchCV(GradientBoostingRegressor(random_state=42), param_grid, n_jobs=-1))\n",
    "                       ])\n",
    "pipe_grad_boost.fit(X_train, y_train)\n",
    "\n",
    "# Extrahiere das beste Modell und die besten Parameter\n",
    "best_params = pipe_grad_boost.named_steps[\"gs_grad_boost\"].best_params_\n",
    "print(\"Beste Parameter:\", best_params)\n",
    "\n",
    "y_pred = pipe_grad_boost.predict(X_test)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "y_pred = pipe_grad_boost.predict(df_merged_test1)\n",
    "df_merged_test1[\"Solar_MWh_pred\"] = y_pred\n",
    "y_pred = pipe_grad_boost.predict(df_merged_test2)\n",
    "df_merged_test2[\"Solar_MWh_pred\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(50.74023095252428)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_rmse = rmse\n",
    "old_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_test1.to_pickle('test1.pkl')\n",
    "df_merged_test2.to_pickle('test2.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
